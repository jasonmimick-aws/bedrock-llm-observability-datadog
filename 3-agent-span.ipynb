{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "LLMObs.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a weather forecasting agent\n",
    "\n",
    "In the next cells, we build the logic for a simple agent that can answer questions about the weather. The code for the agent is adapted from Peter Roelants's excellent blog post [\"Implement a simple ReAct Agent using OpenAI function calling\"](https://peterroelants.github.io/posts/react-openai-function-calling/).\n",
    "\n",
    "First, we create a system prompt that initiates some basic ReAct agent logic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant who can answer multistep questions by sequentially calling functions. \n",
    "\n",
    "Follow a pattern of:\n",
    "THOUGHT (reason step-by-step about which function to call next),\n",
    "ACTION (call a function to as a next step towards the final answer), \n",
    "OBSERVATION (output of the function).\n",
    "\n",
    "Reason step by step which actions to take to get to the answer. \n",
    "Only call functions with arguments coming verbatim from the user or the output of other functions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_initial_messages(question_prompt):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question_prompt,\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some tools that we want the agent to have access to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "FORECAST_API_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "CURRENT_LOCATION_BY_IP_URL = \"http://ip-api.com/json?fields=lat,lon\"\n",
    "\n",
    "\n",
    "def get_current_location():\n",
    "    time.sleep(0.5)  # simulate a longer task\n",
    "    print(requests.get(CURRENT_LOCATION_BY_IP_URL).json())\n",
    "    return json.dumps(requests.get(CURRENT_LOCATION_BY_IP_URL).json())\n",
    "\n",
    "\n",
    "def get_current_weather(latitude, longitude, temperature_unit):\n",
    "    time.sleep(0.3)  # simulate a longer task\n",
    "    resp = requests.get(\n",
    "        FORECAST_API_URL,\n",
    "        params={\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"temperature_unit\": temperature_unit,\n",
    "            \"current_weather\": True,\n",
    "        },\n",
    "    )\n",
    "    return json.dumps(resp.json())\n",
    "\n",
    "\n",
    "def calculate(formula):\n",
    "    return str(eval(formula))\n",
    "\n",
    "\n",
    "class StopException(Exception):\n",
    "    \"\"\"\n",
    "    Signal that the task is finished.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def finish(answer):\n",
    "    raise StopException(answer)\n",
    "\n",
    "\n",
    "available_functions = {\n",
    "    \"get_current_location\": get_current_location,\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "    \"calculate\": calculate,\n",
    "    \"finish\": finish,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a JSON schema in the `function_schema` array to describe each function. We'll pass this to the agent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_schema = [\n",
    "    {\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_location\",\n",
    "            \"description\": \"Get the current location of the user.\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []},\n",
    "        },\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"latitude\": {\"type\": \"number\"},\n",
    "                    \"longitude\": {\"type\": \"number\"},\n",
    "                    \"temperature_unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"latitude\", \"longitude\", \"temperature_unit\"],\n",
    "            },\n",
    "        },\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Calculate the result of a given formula.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"formula\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Numerical expression to compute the result of, in Python syntax.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"formula\"],\n",
    "            },\n",
    "        },\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"function\": {\n",
    "            \"name\": \"finish\",\n",
    "            \"description\": \"Once you have the information required, answer the user's original question, and finish the conversation.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"answer\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Answer to the user's question.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"answer\"],\n",
    "            },\n",
    "        },\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate our Amazon Bedrock client:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=os.environ.get(\"AWS_REGION\", \"us-east-1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function called `execute_loop_step` that handles the recursive agent logic. In the function, we:\n",
    "\n",
    "1. Call the Amazon Bedrock invoke_model endpoint with the system and user prompt.\n",
    "2. Execute any tool calls requested by the LLM.\n",
    "3. Add the results of those tool calls to the messages array in the `append_tool_message_and_execute_loop` helper function.\n",
    "4. Call the invoke_model endpoint with the new messages array.\n",
    "5. We stop the loop when we either have an answer, or we've reached the `MAX_CALLS` limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddtrace.llmobs.decorators import *\n",
    "\n",
    "MAX_CALLS = 4\n",
    "MODEL = \"anthropic.claude-v2\"\n",
    "\n",
    "def get_model_id(model_name=\"claude\"):\n",
    "    \"\"\"\n",
    "    Get the model ID based on the model name\n",
    "    Default is Claude-2\n",
    "    \"\"\"\n",
    "    model_mapping = {\n",
    "        \"claude\": \"anthropic.claude-v2\",  # Using Claude-2 as default\n",
    "        \"claude-2\": \"anthropic.claude-v2\",\n",
    "        \"claude-instant\": \"anthropic.claude-instant-v1\"\n",
    "    }\n",
    "    return model_mapping.get(model_name, model_mapping[\"claude\"]),
    "\n",
    "@workflow()\n",
    "def execute_loop_step(messages, calls_left=MAX_CALLS):\n",
    "\n",
    "    if calls_left < 1:\n",
    "        return messages\n",
    "    \n",
    "    # Convert messages to Claude format\n",
    "    claude_prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.get(\"role\") == \"system\":\n",
    "            claude_prompt += f\"{message.get('content')}\\n\\n\"\n",
    "        elif message.get(\"role\") == \"user\":\n",
    "            claude_prompt += f\"Human: {message.get('content')}\\n\\n\"\n",
    "        elif message.get(\"role\") == \"assistant\":\n",
    "            claude_prompt += f\"Assistant: {message.get('content') or ''}\\n\\n\"\n",
    "        elif message.get(\"role\") == \"tool\":\n",
    "            claude_prompt += f\"Tool ({message.get('name')}): {message.get('content')}\\n\\n\"\n",
    "    \n",
    "    claude_prompt += \"Assistant: \"\n",
    "    \n",
    "    # Define tools for Claude\n",
    "    tools = []\n",
    "    for func in function_schema:\n",
    "        tools.append({\n",
    "            \"name\": func[\"function\"][\"name\"],\n",
    "            \"description\": func[\"function\"].get(\"description\", \"\"),\n",
    "            \"parameters\": func[\"function\"].get(\"parameters\", {})\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "            modelId=MODEL,\n",
    "            body=json.dumps({\n",
    "                \"prompt\": claude_prompt,\n",
    "                \"max_tokens_to_sample\": 2000,\n",
    "                \"temperature\": 0,\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"tools\": tools\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        completion = response_body.get('completion', '')\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(completion)\n",
    "        \n",
    "        # Check for function calls in the response\n",
    "        tool_calls = []\n",
    "        if '<function_call>' in completion and '</function_call>' in completion:\n",
    "            # Extract content between function call tags\n",
    "            start_idx = completion.find('<function_call>')\n",
    "            end_idx = completion.find('</function_call>')\n",
    "            function_content = completion[start_idx:end_idx + len('</function_call>')]\n",
    "            \n",
    "            # Parse the function name and arguments\n",
    "            lines = function_content.strip().split('\\n')\n",
    "            if len(lines) >= 3:  # At least function_call tag, name, and JSON\n",
    "                function_name = lines[1].strip()\n",
    "                json_content = '\\n'.join(lines[2:-1])  # Join all lines between name and closing tag\n",
    "                \n",
    "                try:\n",
    "                    arguments = json.loads(json_content)\n",
    "                    tool_calls.append({\n",
    "                        \"function\": {\n",
    "                            \"name\": function_name,\n",
    "                            \"arguments\": json_content\n",
    "                        }\n",
    "                    })\n",
    "                    print(\"\\n\")\n",
    "                    print(\"CALL TOOL:\", tool_calls)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to parse function arguments: {json_content}\")\n",
    "        \n",
    "        # Create a response message similar to OpenAI format\n",
    "        response_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": completion if not tool_calls else None,\n",
    "            \"tool_calls\": tool_calls if tool_calls else None\n",
    "        }\n",
    "        messages.append(response_message)\n",
    "        \n",
    "        if not tool_calls:\n",
    "            return execute_loop_step(messages, calls_left - 1)\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "        # define a small helper function to reduce repetitive code\n",
    "        def append_tool_message_and_execute_loop(content):\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": content,\n",
    "                }\n",
    "            )\n",
    "            return execute_loop_step(messages, calls_left - 1)\n",
    "\n",
    "        function_name = tool_call.function.name\n",
    "        function_to_call = available_functions[function_name]\n",
    "        if function_to_call is None:\n",
    "            return append_tool_message_and_execute_loop(\n",
    "                f\"Invalid function name: {function_name!r}\"\n",
    "            )\n",
    "        try:\n",
    "            function_args_dict = json.loads(tool_call.function.arguments)\n",
    "        except json.JSONDecodeError as exc:\n",
    "            return append_tool_message_and_execute_loop(\n",
    "                f\"Error decoding function call `{function_name}` arguments {tool_call.function.arguments!r}! Error: {exc!s}\"\n",
    "            )\n",
    "        try:\n",
    "            with LLMObs.tool(function_name):\n",
    "                LLMObs.annotate(input_data=function_args)\n",
    "                try:\n",
    "                    if function_name == \"get_current_weather\":\n",
    "                        location = function_args.get(\"location\")\n",
    "                        unit = function_args.get(\"unit\")\n",
    "                        function_response = get_current_weather(location, unit)\n",
    "                    elif function_name == \"get_n_day_weather_forecast\":\n",
    "                        location = function_args.get(\"location\")\n",
    "                        unit = function_args.get(\"unit\")\n",
    "                        days = function_args.get(\"days\")\n",
    "                        function_response = get_n_day_weather_forecast(location, unit, days)\n",
    "                    else:\n",
    "                        function_response = f\"Error: function {function_name} does not exist\"\n",
    "                    LLMObs.annotate(output_data=function_response)\n",
    "                except StopException as answer:\n",
    "                    LLMObs.annotate(output_data=\"StopException\")\n",
    "                    return str(answer)\n",
    "            return append_tool_message_and_execute_loop(json.dumps(function_response))\n",
    "        except Exception as exc:\n",
    "            return append_tool_message_and_execute_loop(\n",
    "                f\"Error calling function `{function_name}`: {type(exc).__name__}: {exc!s}!\"\n",
    "            )\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Bedrock: {str(e)}\")\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"Error: {str(e)}\"})\n",
    "        return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the top-level function to take a prompt from a user, call the agent, and return a response:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.datadoghq.com/llm_observability/setup/sdk/python/#agent-span\n",
    "@agent()\n",
    "def call_weather_assistant(question_prompt):\n",
    "    LLMObs.annotate(\n",
    "        input_data=question_prompt,\n",
    "    )\n",
    "    messages = get_initial_messages(question_prompt)\n",
    "    answer = execute_loop_step(messages)\n",
    "    LLMObs.annotate(\n",
    "        output_data=answer,\n",
    "    )\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can ask the weather assistant questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CALL TOOL: [ChatCompletionMessageToolCall(id='call_HfZSK8FEQwmVVYRNqLyh9FfH', function=Function(arguments='{}', name='get_current_location'), type='function')]\n",
      "{'lat': 41.7599, 'lon': -72.7574}\n",
      "\n",
      "\n",
      "THOUGHT\n",
      "Now that I have obtained the user's current location coordinates, I will use these coordinates to get the weather data at that location. The user wants the temperature in Fahrenheit, so I will specify that in the function.\n",
      "\n",
      "ACTION\n",
      "Call the function to get the current weather of the user's location.\n",
      "\n",
      "\n",
      "CALL TOOL: [ChatCompletionMessageToolCall(id='call_md31coobmrNryqsSj0AqgO7s', function=Function(arguments='{\\n\"latitude\": 41.7599,\\n\"longitude\": -72.7574,\\n\"temperature_unit\": \"fahrenheit\"\\n}', name='get_current_weather'), type='function')]\n",
      "\n",
      "\n",
      "THOUGHT\n",
      "Now that I have the weather data, I will respond to the user with the temperature and their location coordinates. \n",
      "\n",
      "ACTION\n",
      "Finish the request and provide the requested data to the user.\n",
      "\n",
      "\n",
      "CALL TOOL: [ChatCompletionMessageToolCall(id='call_NqjgoEklHjHeaLSN3sEcaRnj', function=Function(arguments='{\\n\"answer\": \"The current temperature in your location (41.7599, -72.7574) is 95.6°F.\"\\n}', name='finish'), type='function')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current temperature in your location (41.7599, -72.7574) is 95.6°F.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_weather_assistant(\n",
    "    \"What is the weather in my current location? Please give me the temperature in farenheit. Also tell me my current location coordinates.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the trace in Datadog\n",
    "\n",
    "Now, try checking out the [LLM Observability interface](https://app.datadoghq.com/llm) in Datadog. You should see a trace that describes the agent we just ran.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
